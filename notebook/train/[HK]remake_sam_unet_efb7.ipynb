{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[HK]remake_sam_unet_efb7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1GlDSZ3c3vO8I3Sf_CYhh5zOhs6ocAzA4",
      "authorship_tag": "ABX9TyNJxazkpIsVScEkWWjQkPj5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c7810f86814c458d869170ccc7c4bc97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f373820657a649a69fba4c51a362b0d4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_499166ad845c4b299f546117b128c701",
              "IPY_MODEL_06051c6c56ca470f8143fce185bde5b1"
            ]
          }
        },
        "f373820657a649a69fba4c51a362b0d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "499166ad845c4b299f546117b128c701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1e6338f2a8cd46608d3e3c61fe6bb830",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 266860719,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 266860719,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_965b8a1fb11e4c90a1d64baad4e1a324"
          }
        },
        "06051c6c56ca470f8143fce185bde5b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a50aed2f3d3c447180169fdbf0e47358",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 254M/254M [00:05&lt;00:00, 46.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0227b7e95c9447c8dce215a49c339b9"
          }
        },
        "1e6338f2a8cd46608d3e3c61fe6bb830": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "965b8a1fb11e4c90a1d64baad4e1a324": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a50aed2f3d3c447180169fdbf0e47358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0227b7e95c9447c8dce215a49c339b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deagwon97/algorithm/blob/master/notebook/train/%5BHK%5Dremake_sam_unet_efb7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C9k3k7HieMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8209a99-38e2-4dff-be45-098c4e0c5334"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install segmentation_models_pytorch\n",
        "!pip install albumentations==0.3.2\n",
        "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir # albu.lambda 지원을 위해 설치\n",
        "!pip install wandb\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting segmentation_models_pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/54/8953f9f7ee9d451b0f3be8d635aa3a654579abf898d17502a090efe1155a/segmentation_models_pytorch-0.1.3-py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.5MB/s \n",
            "\u001b[?25hCollecting timm==0.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/2d/39ecc56fbb202e1891c317e8e44667299bc3b0762ea2ed6aaaa2c2f6613c/timm-0.3.2-py3-none-any.whl (244kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from segmentation_models_pytorch) (0.8.1+cu101)\n",
            "Collecting efficientnet-pytorch==0.6.3\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/cb/0309a6e3d404862ae4bc017f89645cf150ac94c14c88ef81d215c8e52925/efficientnet_pytorch-0.6.3.tar.gz\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.6/dist-packages (from timm==0.3.2->segmentation_models_pytorch) (1.7.0+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (1.19.4)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.3.2->segmentation_models_pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.3.2->segmentation_models_pytorch) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.3.2->segmentation_models_pytorch) (0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-cp36-none-any.whl size=12421 sha256=49b4df94dbe348c178f071a7e276a6312a46b18c19ad85bf30b6e6c3f328355a\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/1e/a9/2a578ba9ad04e776e80bf0f70d8a7f4c29ec0718b92d8f6ccd\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60963 sha256=c85cbcda213c2b75c2df2932f7f3995ce8419edfb117489a0bce555815262d5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: timm, efficientnet-pytorch, munch, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.3 timm-0.3.2\n",
            "Collecting albumentations==0.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/34/e1da4fab7282d732a6cef827c7e5fb1efa1f02c3ba1bff4a0ace2daf6639/albumentations-0.3.2.tar.gz (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.3.2) (1.19.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.3.2) (1.4.1)\n",
            "Collecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/fc/4da675cc522a749ebbcf85c5a63fba844b2d44c87e6f24e3fdb147df3270/opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6MB)\n",
            "\u001b[K     |████████████████████████████████| 37.6MB 145kB/s \n",
            "\u001b[?25hCollecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 71.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.3.2) (3.13)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.3.2) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.2) (7.0.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.2) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.2) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.2) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.2) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.2) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.2) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.2) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.2) (2.4.7)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.3.2) (4.4.2)\n",
            "Building wheels for collected packages: albumentations, imgaug\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.3.2-cp36-none-any.whl size=51062 sha256=cbbcb7b96ce8b5030cc01a9573a54e790f035263785e577fb2dff3c9f459f3b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/74/a9/b8cfb94bcf1a5d7ea53a6b522bcd372b23b64595b7328e4f3f\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=fcd4a44debb5500b3bfa7a51a800b09950b8321998e2473941e6f96e0d54761d\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built albumentations imgaug\n",
            "Installing collected packages: opencv-python-headless, imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.3.2 imgaug-0.2.6 opencv-python-headless-4.5.1.48\n",
            "Collecting git+https://github.com/albu/albumentations\n",
            "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-e9_6l_bh\n",
            "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-e9_6l_bh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbmnJOf0isGx"
      },
      "source": [
        "from PIL import Image\n",
        "import tifffile as tiff\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm as tqdm\n",
        "import random\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "import wandb\n",
        "import math\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
        "\n",
        "import albumentations as albu\n",
        "#from albumentations import torch as AT\n",
        "from albumentations import Compose,Resize,OneOf,RandomBrightness,RandomContrast,Normalize,HorizontalFlip,Blur,ElasticTransform,GridDistortion,OpticalDistortion,GaussNoise \n",
        "from albumentations.pytorch import ToTensor\n",
        "import segmentation_models_pytorch as smp\n",
        "train_meta = pd.read_csv('/content/drive/MyDrive/kaggledrive/kidney/new_data/train.csv').set_index('id')\n",
        "\n",
        "seed = 1015\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if not os.path.exists(\"train_object_512\"):\n",
        "    !mkdir /content/train_object_512\n",
        "    !mkdir /content/masks_object_512\n",
        "\n",
        "    path = \"/content/drive/MyDrive/kaggledrive/kidney/remake_data2/\"\n",
        "    !cp \"{path}train_object_512.zip\" \"/content/train_object_512/train_object.zip\"\n",
        "    !cp \"{path}masks_object_512.zip\" \"/content/masks_object_512/masks_object.zip \"\n",
        "\n",
        "    !unzip \"/content/train_object_512/train_object.zip\"  -d \"/content/train_object_512\"\n",
        "    !unzip \"/content/masks_object_512/masks_object.zip \" -d \"/content/masks_object_512\"\n",
        "\n",
        "    clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzwTYuc6jhEB"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/kaggledrive/kidney/adaboosting/fp_img.zip\"  \"fp_img.zip\"\r\n",
        "!cp \"/content/drive/MyDrive/kaggledrive/kidney/adaboosting/fp_mask.zip\" \"fp_mask.zip\"\r\n",
        "\r\n",
        "!cp \"/content/drive/MyDrive/kaggledrive/kidney/adaboosting/fn_img.zip\"  \"fn_img.zip\"\r\n",
        "!cp \"/content/drive/MyDrive/kaggledrive/kidney/adaboosting/fn_mask.zip\" \"fn_mask.zip\"\r\n",
        "\r\n",
        "if not os.path.exists(\"fp_img\"):\r\n",
        "    !mkdir /content/fp_img\r\n",
        "    !mkdir /content/fp_mask\r\n",
        "    !mkdir /content/fn_img\r\n",
        "    !mkdir /content/fn_mask   \r\n",
        "\r\n",
        "    !unzip \"fp_img.zip\"   -d \"/content/fp_img\" \r\n",
        "    !unzip \"fp_mask.zip\"  -d \"/content/fp_mask\" \r\n",
        "    !unzip \"fn_img.zip\"   -d \"/content/fn_img\" \r\n",
        "    !unzip \"fn_mask.zip\"  -d \"/content/fn_mask\" \r\n",
        "    clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhXNSMntk2XX"
      },
      "source": [
        "fp_img = []\r\n",
        "with zipfile.ZipFile(\"/content/fp_img.zip\", 'r') as img_arch:\r\n",
        "    fp_img_list = img_arch.namelist().copy()\r\n",
        "fp_list = list(map(lambda x: str(x)[4:],fp_img_list))\r\n",
        "\r\n",
        "\r\n",
        "fn_img = []\r\n",
        "with zipfile.ZipFile(\"/content/fn_img.zip\", 'r') as img_arch:\r\n",
        "    fn_img_list = img_arch.namelist().copy()\r\n",
        "fn_list = list(map(lambda x: str(x)[4:],fn_img_list))\r\n",
        "\r\n",
        "print(len(fn_list))\r\n",
        "print(len(fp_img_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDPhk9ZIl2Cp"
      },
      "source": [
        "print(len(fp_img_list) / len(namelist))\r\n",
        "print(len(fn_img_list) / len(namelist))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rl2kXiOi7Ff"
      },
      "source": [
        "class Kidney_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, namelist, \n",
        "                 transform=None,\n",
        "                 preprocessing=None,\n",
        "                 classes=1, \n",
        "                 augmentation=None, \n",
        "                ):\n",
        "        self.namelist = namelist\n",
        "        self.transforms = transform\n",
        "        self.classes = classes\n",
        "        self.preprocessing = preprocessing\n",
        "        self.augmentation = augmentation\n",
        "        self.imgsize = 512\n",
        "\n",
        "        self.to_tensor = Compose([\n",
        "                                  albu.Lambda(image= to_tensor, mask=to_tensor),\n",
        "                               ])\n",
        "        \n",
        "        \n",
        "    def __getitem__(self,index):\n",
        "        img_name = self.namelist[index]\n",
        "        # Read Data----------------------------------------\n",
        "        if np.random.rand(1)[0] < 0.15:\n",
        "        #if np.random.rand(1)[0] > 0:\n",
        "            if np.random.rand(1)[0] < 0.6:\n",
        "                fp_idx = np.random.randint(len(fp_list))\n",
        "                img = np.array(Image.open(f\"fp_img/img_{fp_list[fp_idx]}\"))\n",
        "                mask = np.array(Image.open(f\"fp_mask/mask_{fp_list[fp_idx]}\"))[...,np.newaxis]\n",
        "                                \n",
        "            else:\n",
        "                fn_idx = np.random.randint(len(fn_list))\n",
        "                img = np.array(Image.open(f\"fn_img/img_{fn_list[fn_idx]}\"))\n",
        "                mask = np.array(Image.open(f\"fn_mask/mask_{fn_list[fn_idx]}\"))[...,np.newaxis]\n",
        "        else:\n",
        "            img = np.array(Image.open(\"train_object_512/\"+img_name))\n",
        "            mask = np.array(Image.open(\"masks_object_512/\"+img_name))[...,np.newaxis]\n",
        "\n",
        "\n",
        "        #apply augmentation\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=img, mask=mask)\n",
        "            img, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=img, mask=mask)\n",
        "            img, mask = sample['image'], sample['mask']\n",
        "\n",
        "        # reshape for converting to tensor (모두 적용)\n",
        "        sample = self.to_tensor(image=img, mask=mask)\n",
        "        img, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        return img/255, mask\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.namelist)\n",
        "\n",
        "def get_training_augmentation():\n",
        "    transform = [\n",
        "        albu.Transpose(p=0.5),   # 2배\n",
        "        albu.RandomRotate90(3),  # 4배\n",
        "        albu.ShiftScaleRotate(p = 1),\n",
        "        albu.ColorJitter(p = 0.8),\n",
        "        #albu.CLAHE(p = 0.5),\n",
        "        #albu.RandomSizedCrop(min_max_height=(384, 512), height = 512, width = 512, p = 1),#(128,256)사이에서 랜덤하게 크롭하고 회전\n",
        "        albu.GridDistortion(p = 0.2), # 2배\n",
        "        albu.GridDropout(p=0.5, ratio = 0.3),\n",
        "        ]\n",
        "    return albu.Compose(transform)\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    _transform = [\n",
        "        albu.Lambda(image = preprocessing_fn),\n",
        "    ]\n",
        "    return albu.Compose(_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrOIPvSjjJuj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIyBCKRsP90y"
      },
      "source": [
        "namelist = []\r\n",
        "with zipfile.ZipFile(\"/content/train_object_512/train_object.zip\", 'r') as img_arch:\r\n",
        "    namelist = img_arch.namelist().copy()\r\n",
        "len(namelist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UwrGM9kjM6E"
      },
      "source": [
        "dataset = Kidney_Dataset(namelist, augmentation = get_training_augmentation(),  classes=1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSRBx07FjON9"
      },
      "source": [
        "img_id = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LrQMcxPjQCl"
      },
      "source": [
        "img_id = 15 +img_id\n",
        "plt.figure(figsize=(10,10))\n",
        "img, mask = dataset[img_id]\n",
        "plt.imshow(img.transpose([1,2,0])) # 원본 # permute는 축 변경\n",
        "plt.imshow(mask[0,:,:], alpha=0.3) # 레이블\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km_Xbugv0rWn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaF1hTIpnuyJ"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "class SAM(torch.optim.Optimizer):\r\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\r\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\r\n",
        "\r\n",
        "        defaults = dict(rho=rho, **kwargs)\r\n",
        "        super(SAM, self).__init__(params, defaults)\r\n",
        "\r\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\r\n",
        "        self.param_groups = self.base_optimizer.param_groups\r\n",
        "\r\n",
        "    @torch.no_grad()\r\n",
        "    def first_step(self, zero_grad=False):\r\n",
        "        grad_norm = self._grad_norm()\r\n",
        "        for group in self.param_groups:\r\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\r\n",
        "\r\n",
        "            for p in group[\"params\"]:\r\n",
        "                if p.grad is None: continue\r\n",
        "                e_w = p.grad * scale\r\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\r\n",
        "                self.state[p][\"e_w\"] = e_w\r\n",
        "\r\n",
        "        if zero_grad: self.zero_grad()\r\n",
        "\r\n",
        "    @torch.no_grad()\r\n",
        "    def second_step(self, zero_grad=False):\r\n",
        "        for group in self.param_groups:\r\n",
        "            for p in group[\"params\"]:\r\n",
        "                if p.grad is None: continue\r\n",
        "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\r\n",
        "\r\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\r\n",
        "\r\n",
        "        if zero_grad: self.zero_grad()\r\n",
        "\r\n",
        "    def step(self, closure=None):\r\n",
        "        raise NotImplementedError(\"SAM doesn't work like the other optimizers, you should first call `first_step` and the `second_step`; see the documentation for more info.\")\r\n",
        "\r\n",
        "    def _grad_norm(self):\r\n",
        "        norm = torch.norm(\r\n",
        "                    torch.stack([\r\n",
        "                        p.grad.norm(p=2)\r\n",
        "                        for group in self.param_groups for p in group[\"params\"]\r\n",
        "                        if p.grad is not None\r\n",
        "                    ]),\r\n",
        "                    p=2\r\n",
        "               )\r\n",
        "        return norm\r\n",
        "\r\n",
        "\r\n",
        "def seed_everything(seed):\r\n",
        "    random.seed(seed)\r\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "SEED = 0\r\n",
        "seed_everything(SEED)\r\n",
        "\r\n",
        "\r\n",
        "class DiceScore(smp.utils.base.Metric):\r\n",
        "    __name__ = 'dice_score'\r\n",
        "\r\n",
        "    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.eps = eps\r\n",
        "        self.threshold = threshold\r\n",
        "        #self.activation = Activation(activation)\r\n",
        "        self.ignore_channels = ignore_channels\r\n",
        "\r\n",
        "    def forward(self, y_pr, y_gt):\r\n",
        "        y_pr = smp.utils.functional._threshold(y_pr, threshold=self.threshold)\r\n",
        "        y_pr, y_gt = smp.utils.functional._take_channels(y_pr, y_gt, ignore_channels=self.ignore_channels)\r\n",
        "        tp = torch.sum(y_gt * y_pr)\r\n",
        "        total = torch.sum(y_gt) + torch.sum(y_pr)\r\n",
        "        score = (tp + self.eps) * 2 / (total + self.eps)\r\n",
        "        return score\r\n",
        "\r\n",
        "\r\n",
        "class DiceBCELoss(smp.utils.base.Loss):\r\n",
        "    __name__ = 'dice_bce_loss'\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.diceloss = smp.utils.losses.DiceLoss()\r\n",
        "        self.bceloss  = torch.nn.BCELoss()\r\n",
        "        self.bceloss.__name__ = 'bce_loss'\r\n",
        "    def forward(self, y_pr, y_gt):\r\n",
        "        return 0.4 * self.diceloss.forward(y_pr, y_gt) + 0.6 * self.bceloss.forward(y_pr, y_gt)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class SamTrainEpoch(smp.utils.train.TrainEpoch):\r\n",
        "    def __init__(self, model, loss, metrics, optimizer, device='cuda', verbose=True):\r\n",
        "        super().__init__(\r\n",
        "            model = model,\r\n",
        "            loss = loss,\r\n",
        "            metrics = metrics,\r\n",
        "            optimizer = optimizer,\r\n",
        "            device = device,\r\n",
        "            verbose = verbose\r\n",
        "        )\r\n",
        "\r\n",
        "    def batch_update(self, x, y):\r\n",
        "        self.optimizer.zero_grad()\r\n",
        "        prediction = self.model.forward(x)\r\n",
        "\r\n",
        "        #\r\n",
        "        loss = self.loss(prediction, y)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.first_step(zero_grad=True)\r\n",
        "\r\n",
        "        self.loss(self.model.forward(x), y).backward()\r\n",
        "        optimizer.second_step(zero_grad=True)\r\n",
        "\r\n",
        "        return loss, prediction\r\n",
        "\r\n",
        "    def run(self, dataloader):\r\n",
        "\r\n",
        "        self.on_epoch_start()\r\n",
        "\r\n",
        "        logs = {}\r\n",
        "        loss_meter = smp.utils.meter.AverageValueMeter()\r\n",
        "        metrics_meters = {metric.__name__: smp.utils.meter.AverageValueMeter() for metric in self.metrics}\r\n",
        "\r\n",
        "        with tqdm(dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as iterator:\r\n",
        "            \r\n",
        "            for x, y in iterator:\r\n",
        "                x, y = x.to(self.device), y.to(self.device)\r\n",
        "                loss, y_pred = self.batch_update(x, y)\r\n",
        "\r\n",
        "                # update loss logs\r\n",
        "                loss_value = loss.cpu().detach().numpy()\r\n",
        "                loss_meter.add(loss_value)\r\n",
        "                loss_logs = {self.loss.__name__: loss_meter.mean}\r\n",
        "                logs.update(loss_logs)\r\n",
        "\r\n",
        "                # update metrics logs\r\n",
        "                for metric_fn in self.metrics:\r\n",
        "                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()\r\n",
        "                    metrics_meters[metric_fn.__name__].add(metric_value)\r\n",
        "                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}\r\n",
        "                logs.update(metrics_logs)\r\n",
        "\r\n",
        "                new_logs = {}\r\n",
        "                for key in logs.keys():\r\n",
        "                    new_logs['train_'+key] = logs[key]\r\n",
        "                wandb.log(new_logs)\r\n",
        "\r\n",
        "                if self.verbose:\r\n",
        "                    s = self._format_logs(logs)\r\n",
        "                    iterator.set_postfix_str(s)\r\n",
        "        return logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjHKvNWWw9aX"
      },
      "source": [
        "namelist = []\r\n",
        "with zipfile.ZipFile(\"/content/train_object_512/train_object.zip\", 'r') as img_arch:\r\n",
        "    namelist = img_arch.namelist().copy()\r\n",
        "len(namelist)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRRTSfy5438q"
      },
      "source": [
        "# model save path\r\n",
        "path = '/content/drive/MyDrive/kaggledrive/kidney/model/adaboost/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "29cblNfz_1GB",
        "outputId": "3f89a115-8358-4ec7-c203-0644cf834e23"
      },
      "source": [
        "import wandb\r\n",
        "!wandb login\r\n",
        "# 04e6d67fe4bcbf0d12c5ec8957027496271c0245\r\n",
        "wandb.init(project=\"hubmap_kidney\", reinit = True)\r\n",
        "train_name = 'adaboost'\r\n",
        "wandb.run.name = train_name\r\n",
        "wandb.run.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeagwon-bu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.12<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">fresh-spaceship-39</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/deagwon-bu/hubmap_kidney\" target=\"_blank\">https://wandb.ai/deagwon-bu/hubmap_kidney</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/deagwon-bu/hubmap_kidney/runs/3c63nntn\" target=\"_blank\">https://wandb.ai/deagwon-bu/hubmap_kidney/runs/3c63nntn</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210101_064031-3c63nntn</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFXSnboNjwJ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982,
          "referenced_widgets": [
            "c7810f86814c458d869170ccc7c4bc97",
            "f373820657a649a69fba4c51a362b0d4",
            "499166ad845c4b299f546117b128c701",
            "06051c6c56ca470f8143fce185bde5b1",
            "1e6338f2a8cd46608d3e3c61fe6bb830",
            "965b8a1fb11e4c90a1d64baad4e1a324",
            "a50aed2f3d3c447180169fdbf0e47358",
            "e0227b7e95c9447c8dce215a49c339b9"
          ]
        },
        "outputId": "8c94c9b9-cf4a-4d6d-fc81-2185e656c870"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "namelist = np.array(namelist)\n",
        "train_names, test_names = train_test_split(namelist, test_size=0.1,random_state=1015)\n",
        "\n",
        "target_fold_index = 1\n",
        "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(namelist),1):\n",
        "    print(fold_index)\n",
        "    if fold_index == target_fold_index:\n",
        "        train_names, test_names = namelist[trn_idx], namelist[val_idx]\n",
        "\n",
        "        ENCODER = 'efficientnet-b7'\n",
        "        ENCODER_WEIGHTS = 'imagenet'\n",
        "        DEVICE = 'cuda'\n",
        "        ACTIVATION = 'sigmoid'\n",
        "\n",
        "        model = smp.Unet(\n",
        "                    encoder_name=ENCODER, \n",
        "                    encoder_weights=ENCODER_WEIGHTS, \n",
        "                    in_channels = 3,\n",
        "                    classes=1, \n",
        "                    activation = ACTIVATION\n",
        "                )\n",
        "        #loss = smp.utils.losses.DiceLoss()\n",
        "        loss = DiceBCELoss()\n",
        "        # 마찬가지로 mae_over_fscore의 pytorch metric version입니다.\n",
        "        metrics = [ DiceScore(threshold=0.5)]\n",
        "        #base_optimizer = Ralamb(model.parameters(), weight_decay=0.01)\n",
        "        #optimizer = Lookahead(base_optimizer)\n",
        "        base_optimizer = torch.optim.SGD\n",
        "        optimizer = SAM(model.parameters(), base_optimizer, lr=0.1, momentum=0.9)\n",
        "\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 10, 2,eta_min=1e-6) # 1e-6\n",
        "\n",
        "        train_epoch = SamTrainEpoch(\n",
        "            model, \n",
        "            loss=loss, \n",
        "            metrics=metrics, \n",
        "            optimizer=optimizer,\n",
        "            device=DEVICE,\n",
        "            verbose=True,\n",
        "        )\n",
        "\n",
        "        valid_epoch = smp.utils.train.ValidEpoch(\n",
        "            model, \n",
        "            loss=loss, \n",
        "            metrics=metrics, \n",
        "            device=DEVICE,\n",
        "            verbose=True,\n",
        "        )\n",
        "\n",
        "        BATCH_SIZE = 4\n",
        "        train_dataset = Kidney_Dataset(train_names, augmentation = get_training_augmentation(),  classes=1)\n",
        "        valid_dataset = Kidney_Dataset(test_names, classes=1)\n",
        "        train_loader = DataLoader(train_dataset,  batch_size=BATCH_SIZE, shuffle=True, num_workers=4, persistent_workers = True)\n",
        "        valid_loader = DataLoader(valid_dataset,  batch_size=20, shuffle=False, num_workers=2)\n",
        "\n",
        "        # 한 폴드당 150 Epoch 수행\n",
        "        NUM_EPOCH = 10\n",
        "        min_score = np.Inf\n",
        "\n",
        "        MODEL = train_name\n",
        "        for i in range(0, NUM_EPOCH):\n",
        "            print('\\nEpoch: {}'.format(i))\n",
        "            train_logs = train_epoch.run(train_loader)\n",
        "            valid_logs = valid_epoch.run(valid_loader)\n",
        "\n",
        "            new_logs = {}\n",
        "            for key in valid_logs.keys():\n",
        "                new_logs['valid_'+key] = valid_logs[key]\n",
        "            wandb.log(new_logs)\n",
        "\n",
        "\n",
        "            scheduler.step()\n",
        "            valid_loss = valid_logs['dice_bce_loss']\n",
        "            torch.save(model, f'{path}{fold_index}_{MODEL}_{loss.__name__}_{valid_loss:2.4f}_epoch_{i}.pth')\n",
        "    \n",
        "            print('Model saved!')\n",
        "        target_fold_index += 1\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7810f86814c458d869170ccc7c4bc97",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=266860719.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch: 0\n",
            "train: 100%|██████████| 1281/1281 [33:28<00:00,  1.57s/it, dice_bce_loss - 0.1606, dice_score - 0.7623]\n",
            "valid: 100%|██████████| 65/65 [00:55<00:00,  1.17it/s, dice_bce_loss - 0.08579, dice_score - 0.9061]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 1\n",
            "train: 100%|██████████| 1281/1281 [33:32<00:00,  1.57s/it, dice_bce_loss - 0.08355, dice_score - 0.8929]\n",
            "valid: 100%|██████████| 65/65 [00:55<00:00,  1.18it/s, dice_bce_loss - 0.06823, dice_score - 0.9129]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 2\n",
            "train: 100%|██████████| 1281/1281 [33:35<00:00,  1.57s/it, dice_bce_loss - 0.07623, dice_score - 0.9017]\n",
            "valid: 100%|██████████| 65/65 [00:55<00:00,  1.18it/s, dice_bce_loss - 0.06369, dice_score - 0.9171]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 3\n",
            "train: 100%|██████████| 1281/1281 [33:35<00:00,  1.57s/it, dice_bce_loss - 0.07551, dice_score - 0.9]\n",
            "valid: 100%|██████████| 65/65 [00:55<00:00,  1.18it/s, dice_bce_loss - 0.06075, dice_score - 0.9193]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 4\n",
            "train: 100%|██████████| 1281/1281 [33:34<00:00,  1.57s/it, dice_bce_loss - 0.06888, dice_score - 0.9077]\n",
            "valid: 100%|██████████| 65/65 [00:55<00:00,  1.18it/s, dice_bce_loss - 0.0611, dice_score - 0.9226]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 5\n",
            "train: 100%|██████████| 1281/1281 [33:34<00:00,  1.57s/it, dice_bce_loss - 0.06681, dice_score - 0.9097]\n",
            "valid: 100%|██████████| 65/65 [00:55<00:00,  1.18it/s, dice_bce_loss - 0.05626, dice_score - 0.9266]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 6\n",
            "train: 100%|██████████| 1281/1281 [33:36<00:00,  1.57s/it, dice_bce_loss - 0.06344, dice_score - 0.9138]\n",
            "valid: 100%|██████████| 65/65 [00:55<00:00,  1.18it/s, dice_bce_loss - 0.05474, dice_score - 0.9269]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 7\n",
            "train: 100%|██████████| 1281/1281 [33:36<00:00,  1.57s/it, dice_bce_loss - 0.06164, dice_score - 0.9164]\n",
            "valid: 100%|██████████| 65/65 [00:55<00:00,  1.18it/s, dice_bce_loss - 0.05423, dice_score - 0.9277]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 8\n",
            "train: 100%|██████████| 1281/1281 [33:37<00:00,  1.57s/it, dice_bce_loss - 0.06305, dice_score - 0.9142]\n",
            "valid: 100%|██████████| 65/65 [00:55<00:00,  1.18it/s, dice_bce_loss - 0.05321, dice_score - 0.9291]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 9\n",
            "train: 100%|██████████| 1281/1281 [33:31<00:00,  1.57s/it, dice_bce_loss - 0.05958, dice_score - 0.9188]\n",
            "valid: 100%|██████████| 65/65 [00:55<00:00,  1.18it/s, dice_bce_loss - 0.05308, dice_score - 0.9288]\n",
            "Model saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs6KlMPcY7Rb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jARfXYHTY7N6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X90sgroXY7Lq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSo-r40UY7JY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf7g7rMNY64x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVau_Xc--XVW"
      },
      "source": [
        "checkpoint = { \r\n",
        "    'epoch': i,\r\n",
        "    'model': model.state_dict(),\r\n",
        "    'optimizer': optimizer.state_dict(),\r\n",
        "    'lr_sched': scheduler\r\n",
        "}\r\n",
        "torch.save(checkpoint, path + 'checkpoint.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6khfQ-Dpge-n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}